---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Classifier S - XGBoost (balanced with SMOTE)

## Imports

```{python}
# XGBoost
from xgboost import XGBClassifier
import xgboost as xgb

# SKLearn
from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, StratifiedKFold, KFold, ShuffleSplit
from sklearn.model_selection import StratifiedKFold, LeaveOneOut
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, average_precision_score, recall_score, brier_score_loss, confusion_matrix
from sklearn.calibration import CalibratedClassifierCV, calibration_curve

# Data processing
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt

# Custom functions
from src.func.model_func import get_metrics
from src.func.model_func import save_model

# Imbalanced learn
from imblearn.over_sampling import SMOTE
```

## Steps followed below:

1. Balance the dataset classes using SMOTE
2. Shuffle the extended dataset


## Notebook Reported Results ##

In this notebook we will look at the XGBoost classifier and evaluate its performance with unbalanced data. The data has around *74% of class 1* and *26% of class 0*. <br>

In this notebook we will experiment with different splitting approaches for our test/valid sets after oversampling the minority class with SMOTE. 

1. KFold
2. Stratified KFold

The results can be found below: 

MINORITY/MAJORITY RATIO: 0.4

1. KFold: <br>
    -> ROC AUC (avrg): 0.7924<br>
    -> PR AUC (avrg): 0.8883<br>
    -> Brier Loss: 0.1490<br>
2. Stratified KFold:<br>
    -> ROC AUC (avrg): 0.7450 <br>
    -> PR AUC (avrg): 0.8813<br>
    -> Brier Loss: 0.1621<br>


## Import Data

```{python}
base_path = '/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/'

# data = h2o.import_file('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/data_classifier_s.csv')
# train_data = pd.read_csv('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/train_classifier_s.csv', sep=";")
# test_data = pd.read_csv('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/test_classifier_s.csv', sep=";")

# data = pd.read_csv(base_path+"data/processed/classifier_s/data_classifier_s.csv", sep=";")
# data = all_data.drop(['uuid', 'finished_treatment'],axis=1)

data = pd.read_csv(base_path+"data/processed/data_classifier_s.csv", sep=";")
data = data.fillna(data.mean())

# Add a weight column for the bias correction coefficients
data.insert(data.shape[1]-1, "weight", pd.Series())
```

```{python}
data.head()
```

## Balance with SMOTE

```{python}
X = data.iloc[:, :-2]
y = data.iloc[:, -1]

# Turn on over-sampling 
# The SMOTE parameter defines the ratio of minority to majority class. A smaller ratio can help reduce overfitting
# smote = SMOTE(0.5)
# X, y = smote.fit_resample(X, y)

# over_sampled_data = pd.DataFrame(X, columns=data.columns[0:-2])
# over_sampled_data['got_go']=pd.Series(y)
# over_sampled_data.insert(over_sampled_data.shape[1]-1, "weight", pd.Series())
# over_sampled_data = over_sampled_data.sample(frac=1, random_state=55)

# print("Data shape: {}".format(data.shape))
# print("Data shape: {}".format(over_sampled_data.shape))

# over_sampled_data.tail()
```

```{python}
np.bincount(y)
```

## XGBoost Estimator (Training)

```{python}
def get_best_xgb_estimator(X_train, X_valid, y_train, y_valid, grid_search=False, verbose=False):
    '''
    :param X_train: dataframe or array with training instances
    :param X_test: dataframe or array with test instances
    :param y_train: dataframe or array with training labels
    :param y_test: dataframe or array with testing labels
    :param verbose: set to 'True' for a detailed report of the grid search
    :return: the best estimator and its corresponding score
    '''

    if grid_search == True: 
        
        skf_5 = StratifiedKFold(5, shuffle=True, random_state=1)
        
        # Parameters for the grid search
        parameters = {
            'max_depth': [30],
            'n_estimators': [150],
            'learning_rate':[0.051],
            'reg_lambda':[0.9],
            'objective':['binary:logistic'],
            'eval_metric':["auc"]
        }

        xgbest = GridSearchCV(XGBClassifier(n_jobs=6), param_grid=parameters, cv=skf_5, scoring="roc_auc", verbose=0)
        
    else:
        xgbest = XGBClassifier(n_jobs=6, reg_lambda=0.8, scale_pos_weight=0.4, learing_rate=0.05, max_depth=30, n_estimators=150, objective='binary:logistic')
    
    # Calibrate the classifier here
    calib_xgbest = CalibratedClassifierCV(xgbest, cv=5, method='isotonic')
    calib_xgbest.fit(X_train, y_train)

    predicted_proba = calib_xgbest.predict_proba(X_valid)[:, 1]
    predicted_labels = calib_xgbest.predict(X_valid)
    roc_auc = roc_auc_score(y_valid, predicted_proba)
    pr_auc = average_precision_score(y_valid, predicted_proba)
    
    if verbose:
    
        print(
#             'Best parameters: {} \n'.format(calib_xgbest.best_params_),
            'ROC {} \n'.format(roc_auc),
            'Accuracy {} \n '.format(accuracy_score(y_valid, predicted_labels)),
            'Precision {}\n '.format(precision_score(y_valid, predicted_labels)),
            'Average precision-recall score: {0:0.2f} \n\n'.format(pr_auc)
        )

    return calib_xgbest, roc_auc, pr_auc


def calc_bias_corr_weights(prob_s_positive, predicted_prob_s):
    
    weights = np.zeros(predicted_prob_s.shape[0])
    
    for i in range(0, len(predicted_prob_s)):
        if i < 2479:
            weights[i] = prob_s_positive / predicted_prob_s[i]
        
    return weights
```

```{python}
# Over sampled data
# X = over_sampled_data.iloc[:, :-2]
# y = over_sampled_data.iloc[:, -1]

# Split Approach
# sss = KFold(n_splits=10, random_state=55)
sss = StratifiedKFold(n_splits=10, random_state=55)

sss.get_n_splits(X, y)

# Create SMOTE instance
smote = SMOTE(0.4, random_state=55)

if type(y) == np.ndarray:
    prob_s_pos = np.bincount(y)[1]/len(y)
else:
    prob_s_pos = y.value_counts(True)[1]

roc_aucs = []
pr_aucs = []
best_xgbs = []
splits = []
predicted_prob_s = pd.Series(np.full(y.shape[0], np.nan))

for train_index, valid_index in sss.split(X, y):
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]
    
    # Over sample
    X_train, y_train = smote.fit_resample(X_train, y_train)
    
    # Fit, calibrate and evaluate the classifier
    best_xgb, roc_auc, pr_auc = get_best_xgb_estimator(X_train, X_valid, y_train, y_valid, verbose=False)
    
    # Compute weights for test examples
    predicted_prob_s[X_valid.index] = best_xgb.predict_proba(X_valid)[:, 1]
    
#     i=0
    
#     for index in valid_index:
#         if index < 2479:
#             data.loc[index, "weight"] = prob_s_pos / predicted_prob_s[i]
        
#         i += 1
    
    # Store fold results
    roc_aucs.append(roc_auc)
    pr_aucs.append(pr_auc)
    best_xgbs.append(best_xgb)
    splits.append([train_index, valid_index])
```

```{python}
np.bincount(y_train)[0]/np.bincount(y_train)[1]
```

```{python}
roc = np.array(roc_aucs)
pr = np.array(pr_aucs)

print('Average ROC AUC: ',roc.mean())
print('Average PR AUC: ',pr.mean())
```

```{python}
roc_aucs
```

```{python}
# best_index = 0
best_index = np.argmax(roc_aucs)

print(roc_aucs[best_index])
xgbest = best_xgbs[best_index]
best_splits = splits[best_index]
```

## Evaluation

```{python}
if type(X) == np.ndarray:
    X_test = X[best_splits[1]]
    y_test = y[best_splits[1]]
else:
    X_test = X.iloc[best_splits[1]]
    y_test = y.iloc[best_splits[1]]

predicted_proba = xgbest.predict_proba(X_test)[:, 1]
predicted = xgbest.predict(X_test)

roc_auc = roc_auc_score(y_test, predicted_proba)
pr_auc = average_precision_score(y_test, predicted_proba)

print("ROC AUC: {} \n".format(roc_auc))
print("PR AUC: {} \n".format(pr_auc))
print(confusion_matrix(y_test, predicted))
```

```{python}
def plot_calibration_curve(y_true, y_predicted):
    """Plot calibration curve for est w/o and with calibration. """
    
    fig = plt.figure(1, figsize=(10, 10))
    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
    ax2 = plt.subplot2grid((3, 1), (2, 0))
    
    ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    
    print("Brier Loss: ", brier_score_loss(y_true, y_predicted))
    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_predicted, n_bins=10)

    ax1.plot(mean_predicted_value, fraction_of_positives, "s-")
    ax2.hist(predicted_prob_s, range=(0, 1), bins=10, histtype="step", lw=2)

    ax1.set_ylabel("Fraction of positives")
    ax1.set_ylim([-0.05, 1.05])
    ax1.legend(loc="lower right")
    ax1.set_title('Calibration plots  (reliability curve)')

    ax2.set_xlabel("Mean predicted value")
    ax2.set_ylabel("Count")
    ax2.legend(loc="upper center", ncol=2)

    plt.tight_layout()

# Plot calibration curve for Linear SVC
y_true = y[predicted_prob_s.index]
plot_calibration_curve(y_true, predicted_prob_s)

plt.show()
```

```{python}
# prediction_positive = xgbest.predict_proba(X_test)[:, 1]
plt.hist(predicted_prob_s, bins=10)
```

```{python}
X_test.shape
```

```{python}
data.head()
```

## Save data

```{python}
data["finished_treatment"] = all_data["finished_treatment"]
data_y = data.drop('got_go', axis=1)
```

```{python}
data_y.to_csv("/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_y/weighted/sk_xgb_weights/data_classifier_y_weighted_sk_xgboost.csv", sep=";", index=False)
```

## Save Model

```{python}
# Save model
save_model(best_xgb, '/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/models/classifier_s/calibrated/classifier_s_sk_xgboost')
```

```{python}

```

---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Classifier S with H2O

## Imports

```{python}
# H2O
import h2o

# Data processing
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
```

```{python}
h2o.init()
```

## Import Data

```{python}
data = h2o.import_file('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/data_classifier_s.csv')
train_data = h2o.import_file('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/train_classifier_s.csv')
test_data = h2o.import_file('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/test_classifier_s.csv')
```

```{python}
# Parse categorical data
categorical = ['HulpeloosheidLevel', 'MagnificatieLevel', 'RuminatieLevel', 'TotaalLevel', 'got_go']

for category in categorical:
    train_data[category] = train_data[category].asfactor()
```

```{python}
# Set predictors and response columns
features = train_data.columns
X = features[:-1]
y = features[-1]
```

```{python}
# Split train, test, valid
train, calib, valid = train_data.split_frame([0.7, 0.15])
```

## Distributed Random Forest Estimator (Calibrated)

```{python}
from h2o.estimators.random_forest import H2ORandomForestEstimator
from h2o.grid.grid_search import H2OGridSearch
from sklearn.metrics import accuracy_score
from src.func.model_func import get_metrics
```

```{python}
estimator = H2ORandomForestEstimator(nfolds=5, balance_classes=True, calibrate_model=True, calibration_frame=calib, seed=1234)
```

```{python}
criteria = {
    "strategy": "RandomDiscrete", 
    "stopping_rounds": 10,
    "stopping_tolerance": 0.00001,
    "stopping_metric":"auc"
}

hyper_parameters = {
    'ntrees': [50, 100,200, 250],
    'max_depth': [5, 10, 20, 30, 50, 100],
    'fold_assignment':['Stratified', 'Random']
}

grid_search = H2OGridSearch(estimator, hyper_params=hyper_parameters, search_criteria=criteria)
```

```{python}
grid_search.train(x=X, y=y, training_frame=train, validation_frame=valid)
```

```{python}
sorted_grid = grid_search.get_grid(sort_by='auc',decreasing=True)

best_max_depth  = sorted_grid.sorted_metric_table()['max_depth'][0]
best_ntrees     = sorted_grid.sorted_metric_table()['ntrees'][0]
best_auc        = sorted_grid.sorted_metric_table()['auc'][0]
best_fold_assignment = sorted_grid.sorted_metric_table()['fold_assignment'][0]

print('Best max_depth.....', best_max_depth)
print('Best ntrees........', best_ntrees)
print('Best auc...........', best_auc)
print('Best fold assignment...........', best_fold_assignment)
```

```{python}
sorted_grid.sorted_metric_table().head()
```

```{python}
# Evaluate
best_drf = sorted_grid[0]
predictions = best_drf.predict(test_data).as_data_frame()

accuracy = accuracy_score(test_data.as_data_frame().iloc[:,-1], predictions["predict"])
accuracy
```

```{python}
get_metrics(test_data.as_data_frame().iloc[:,-1], proba_predicted_y=predictions["p1"].values)
```

### Check calibation

```{python}
best_drf = sorted_grid[0]
```

```{python}
predictions = best_drf.predict(test_data)
```

```{python}
calibrated_rates = [predictions['cal_p0'].mean(), predictions['cal_p1'].mean()]
print(calibrated_rates)
```

```{python}
non_calib_rates = [predictions['p0'].mean(), predictions['p1'].mean()]
print(non_calib_rates)
```

```{python}
actual_rates = [data.as_data_frame()['got_go'].value_counts()[0]/data.shape[0], data.as_data_frame()['got_go'].value_counts()[1]/data.shape[0]]
print(actual_rates)
```

```{python}
calib_pred = predictions[:, ['predict', 'cal_p0', 'cal_p1']].as_data_frame()
non_calib_pred = predictions[:, ['predict', 'p0', 'p1']].as_data_frame()
```

```{python}
def get_calibration_diagram(proba_p1, true_classes):
 
    df = pd.DataFrame()
    df['proba_p1'] = proba_p1
    df['true_classes'] = true_classes

    bins = pd.IntervalIndex.from_tuples([(0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5), (0.5, 0.6), (0.6, 0.7), (0.8, 0.9), (0.9, 1.0)])
    df['calib_p1_bins'] = pd.cut(df['proba_p1'], bins)

    average_prob_estimate = []
    obs_class1_freq = []

    for interval, group in df.groupby('calib_p1_bins'):
        size = group.shape[0]

        if size != 0:
            obs_class1_freq.append(group[group['true_classes'] == 1].shape[0]/group.shape[0])
        else:
            obs_class1_freq.append(0)

        interval_proba_mean = group['proba_p1'].mean()

        if type(interval_proba_mean) is float:
            average_prob_estimate.append(0)
        else:
            average_prob_estimate.append(interval_proba_mean)
            
    x,y = zip(*sorted(zip(average_prob_estimate,obs_class1_freq),key=lambda x: x[0]))
    print(x)
    print(y)
    # Plot
    plt.figure()
    plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    plt.plot(x, y, 's-')
    plt.show()
```

```{python}
proba_p1 = calib_pred['cal_p1']
true_classes = data.as_data_frame()['got_go']

get_calibration_diagram(proba_p1, true_classes)
```

```{python}
non_proba_p1 = non_calib_pred['p1']
true_classes = data.as_data_frame()['got_go']

get_calibration_diagram(non_proba_p1, true_classes)
```

## Save Model

```{python}
# model_path = h2o.save_model(model=best_drf, path="/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/models/classifier_s/classifier_s_h2o", force=True)

# load the model
# saved_model = h2o.load_model(model_path)
```

```{python}
model_path
```

---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Classifier S with H2O - DRF

## Imports

```{python}
# H2O
import h2o

# Data processing
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
```

```{python}
h2o.init()
```

## Import Data

```{python}
data = h2o.import_file('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/data_classifier_s.csv')
train_data = h2o.import_file('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/train_classifier_s.csv')
test_data = h2o.import_file('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/test_classifier_s.csv')
```

```{python}
# Parse categorical data
categorical = ['HulpeloosheidLevel', 'MagnificatieLevel', 'RuminatieLevel', 'TotaalLevel', 'got_go']

for category in categorical:
    train_data[category] = train_data[category].asfactor()
```

```{python}
# Set predictors and response columns
features = train_data.columns
X = features[:-1]
y = features[-1]
```

```{python}
# Split train, test, valid
train, valid = train_data.split_frame([0.8])
```

## Distributed Random Forest Estimator (Calibrated)

```{python}
from h2o.estimators.random_forest import H2ORandomForestEstimator
from h2o.grid.grid_search import H2OGridSearch
from sklearn.metrics import accuracy_score
from src.func.model_func import get_metrics
```

```{python}
estimator = H2ORandomForestEstimator(nfolds=5, balance_classes=True, seed=1234)
```

```{python}
hyper_parameters = {
    'ntrees': [50, 100, 200],
    'max_depth': [5, 10, 20, 30, 50],
    'fold_assignment':['Stratified']
}

grid_search = H2OGridSearch(estimator, hyper_params=hyper_parameters)
```

```{python}
grid_search.train(x=X, y=y, training_frame=train, validation_frame=valid)
```

```{python}
sorted_grid = grid_search.get_grid(sort_by='auc',decreasing=True)

best_max_depth  = sorted_grid.sorted_metric_table()['max_depth'][0]
best_ntrees     = sorted_grid.sorted_metric_table()['ntrees'][0]
best_auc        = sorted_grid.sorted_metric_table()['auc'][0]
best_fold_assignment = sorted_grid.sorted_metric_table()['fold_assignment'][0]

print('Best max_depth.....', best_max_depth)
print('Best ntrees........', best_ntrees)
print('Best auc...........', best_auc)
print('Best fold assignment...........', best_fold_assignment)
```

```{python}
sorted_grid.sorted_metric_table().head()
```

```{python}
# Evaluate
best_drf = sorted_grid[0]
predictions = best_drf.predict(test_data).as_data_frame()
```

```{python}
print("Accuracy: ", accuracy_score(test_data.as_data_frame().iloc[:,-1], predictions["predict"]))
get_metrics(test_data.as_data_frame().iloc[:,-1], proba_predicted_y=predictions["p1"].values)
```

```{python}
# model_path = h2o.save_model(model=best_drf, path="/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/models/classifier_s/calibrate/classifier_s_h2o", force=True)

# load the model
# saved_model = h2o.load_model(model_path)

# Current build: /Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/models/classifier_s/calibrated/classifier_s_h2o/Grid_DRF_py_9_sid_8e50_model_python_1552985644490_1_model_37'
```

### Check calibation

```{python}
model_path = "/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/models/classifier_s/calibrated/classifier_s_h2o/Grid_DRF_py_9_sid_8e50_model_python_1552985644490_1_model_37"
model = h2o.load_model(model_path)
```

```{python}
predictions = model.predict(test_data)
```

```{python}
calibrated_rates = [predictions['cal_p0'].mean(), predictions['cal_p1'].mean()]
print(calibrated_rates)
```

```{python}
non_calib_rates = [predictions['p0'].mean(), predictions['p1'].mean()]
print(non_calib_rates)
```

```{python}
actual_rates = [data.as_data_frame()['got_go'].value_counts()[0]/data.shape[0], data.as_data_frame()['got_go'].value_counts()[1]/data.shape[0]]
print(actual_rates)
```

```{python}
calib_pred = predictions[:, ['predict', 'cal_p0', 'cal_p1']].as_data_frame()
non_calib_pred = predictions[:, ['predict', 'p0', 'p1']].as_data_frame()
```

```{python}
from sklearn.calibration import calibration_curve
```

```{python}
y_test = test_data['got_go'].as_data_frame().values
prob_pos = predictions['cal_p1'].as_data_frame().values
non_prob_pos = predictions['p1'].as_data_frame().values

fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)
fraction_of_positives_non, mean_predicted_value_non = calibration_curve(y_test, non_prob_pos, n_bins=10)
fraction_of_positives_non, mean_predicted_value_non = zip(*sorted(zip(fraction_of_positives_non,mean_predicted_value_non),key=lambda x: x[0]))

fig = plt.figure(figsize=(10, 10))
plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="Calibrated")
plt.plot(fraction_of_positives_non, mean_predicted_value_non, "s-", label="Uncalibrated")
plt.legend(loc="best")
non_prob_pos = predictions['p1'].as_data_frame().values
```

```{python}
def get_calibration_data(proba_p1, true_classes):
 
    df = pd.DataFrame()
    df['proba_p1'] = proba_p1
    df['true_classes'] = true_classes

    bins = pd.IntervalIndex.from_tuples([(0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5), (0.5, 0.6), (0.6, 0.7), (0.8, 0.9), (0.9, 1.0)])
    df['calib_p1_bins'] = pd.cut(df['proba_p1'], bins)

    average_prob_estimate = []
    obs_class1_freq = []

    for interval, group in df.groupby('calib_p1_bins'):
        size = group.shape[0]

        if size != 0:
            obs_class1_freq.append(group[group['true_classes'] == 1].shape[0]/group.shape[0])
        else:
            obs_class1_freq.append(0)

        interval_proba_mean = group['proba_p1'].mean()

        if type(interval_proba_mean) is float:
            average_prob_estimate.append(0)
        else:
            average_prob_estimate.append(interval_proba_mean)
            
    x,y = zip(*sorted(zip(average_prob_estimate,obs_class1_freq),key=lambda x: x[0]))
    
    return x,y
    
def get_calibration_diagram(x_calib, y_calib, x_non_calib, y_non_calib):
    # Plot
    plt.figure(figsize=(10,5))
    plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    plt.plot(x_calib, y_calib, 's-')
    plt.plot(x_non_calib, y_non_calib, 's-')
    plt.title("Calibration Diagram")
    plt.xlabel('True Class Percentage')
    plt.ylabel('Predicted Class Probability')
    plt.show()
```

```{python}
true_classes = data.as_data_frame()['got_go']

calib_p1 = calib_pred['cal_p1']
non_calib_p1 = non_calib_pred['p1']

x_calib, y_calib = get_calibration_data(calib_p1, true_classes)
x_non_calib, y_non_calib = get_calibration_data(non_calib_p1, true_classes)

get_calibration_diagram(x_calib, y_calib, x_non_calib, y_non_calib)
```

```{python}

```

---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Classifier S - XGBoost (imbalanced)

## Imports

```{python}
# XGBoost
from xgboost import XGBClassifier
import xgboost as xgb

# SKLearn
from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, StratifiedKFold
from sklearn.model_selection import StratifiedKFold, LeaveOneOut
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, average_precision_score, recall_score, brier_score_loss
from sklearn.calibration import CalibratedClassifierCV, calibration_curve

# Data processing
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt

# Custom functions
from src.func.model_func import get_metrics
from src.func.model_func import save_model

# Imbalanced learn
from imblearn.over_sampling import SMOTE
```

## Import Data

```{python}
# data = h2o.import_file('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/data_classifier_s.csv')
# train_data = pd.read_csv('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/train_classifier_s.csv', sep=";")
# test_data = pd.read_csv('/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_s/test_classifier_s.csv', sep=";")

# base_path = '/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/'
# data = pd.read_csv(base_path+"data/processed/classifier_s/data_classifier_s.csv", sep=";")
# data = all_data.drop(['uuid', 'finished_treatment'],axis=1)

data = pd.read_csv(base_path+"data/processed/data_classifier_s.csv", sep=";")
data = data.fillna(data.mean())

# Add a weight column for the bias correction coefficients
data.insert(data.shape[1]-1, "weight", pd.Series())
```

```{python}
data.head()
```

```{python}
X = data.iloc[:, :-2]
y = data.iloc[:, -1]
```

```{python}
# X, y = SMOTE().fit_resample(X, y)
```

## XGBoost Estimator (Training)

```{python}
def get_best_xgb_estimator(X_train, X_valid, y_train, y_valid, grid_search=False, verbose=False):
    '''
    :param X_train: dataframe or array with training instances
    :param X_test: dataframe or array with test instances
    :param y_train: dataframe or array with training labels
    :param y_test: dataframe or array with testing labels
    :param verbose: set to 'True' for a detailed report of the grid search
    :return: the best estimator and its corresponding score
    '''

    if grid_search == True: 
        
        skf_5 = StratifiedKFold(5, shuffle=True, random_state=1)
        
        # Parameters for the grid search
        parameters = {
            'max_depth': [30],
            'n_estimators': [150],
            'learning_rate':[0.051],
            'reg_lambda':[0.9],
            'objective':['binary:logistic'],
            'eval_metric':["auc"]
        }

        xgbest = GridSearchCV(XGBClassifier(n_jobs=6), param_grid=parameters, cv=skf_5, scoring="roc_auc", verbose=0)
        
    else:
        xgbest = XGBClassifier(n_jobs=6, reg_lambda=0.8, scale_pos_weight=0.9, learing_rate=0.05, max_depth=30, n_estimators=150, objective='binary:logistic')
    
    # Calibrate the classifier here
    calib_xgbest = CalibratedClassifierCV(xgbest, cv=5, method='isotonic')
    calib_xgbest.fit(X_train, y_train)

    predicted_proba = calib_xgbest.predict_proba(X_valid)[:, 1]
    predicted_labels = calib_xgbest.predict(X_valid)
    roc_auc = roc_auc_score(y_valid, predicted_proba)
    pr_auc = average_precision_score(y_valid, predicted_proba)
    
    if verbose:
    
        print(
#             'Best parameters: {} \n'.format(calib_xgbest.best_params_),
            'ROC {} \n'.format(roc_auc),
            'Accuracy {} \n '.format(accuracy_score(y_valid, predicted_labels)),
            'Precision {}\n '.format(precision_score(y_valid, predicted_labels)),
            'Average precision-recall score: {0:0.2f} \n\n'.format(pr_auc)
        )

    return calib_xgbest, roc_auc, pr_auc


def calc_bias_corr_weights(prob_s_positive, predicted_prob_s):
    
    weights = np.zeros(predicted_prob_s.shape[0])
    
    for i in range(0, len(predicted_prob_s)):
        if i < 2479:
            weights[i] = prob_s_positive / predicted_prob_s[i]
        
    return weights
```

```{python}
# prob_s_pos = y.value_counts(True)
# prob_s_pos[1]
```

```{python}
y.value_counts()
```

```{python}
# Stratified K Fold

sss = StratifiedKFold(n_splits=10, random_state=55)
sss.get_n_splits(X, y)

# Turn on over-sampling
X, y = SMOTE().fit_resample(X, y)

if type(y) == np.ndarray:
    prob_s_pos = np.bincount(y)[1]/len(y)
else:
    prob_s_pos = y.value_counts(True)[1]

roc_aucs = []
pr_aucs = []
best_xgbs = []
splits = []
predicted_probas_s = []

for train_index, valid_index in sss.split(X, y):
    if type(X) == np.ndarray:
        X_train, X_valid = X[train_index], X[valid_index]
        y_train, y_valid = y[train_index], y[valid_index]
    else:
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]
    
    # Fit, calibrate and evaluate the classifier
    best_xgb, roc_auc, pr_auc = get_best_xgb_estimator(X_train, X_valid, y_train, y_valid, verbose=False)
    
    # Compute weights for test examples
    predicted_prob_s = best_xgb.predict_proba(X_valid)[:, 1]
    
    i=0
    
    for index in valid_index:
        if index < 2479:
            data.loc[index, "weight"] = prob_s_pos / predicted_prob_s[i]
        
        i += 1
    
    # Store fold results
    roc_aucs.append(roc_auc)
    pr_aucs.append(pr_auc)
    best_xgbs.append(best_xgb)
    splits.append([train_index, valid_index])
    predicted_probas_s.append(predicted_prob_s)
```

```{python}
roc = np.array(roc_aucs)
pr = np.array(pr_aucs)

print('Average ROC AUC: ',roc.mean())
print('Average PR AUC: ',pr.mean())
```

```{python}
roc_aucs
```

```{python}
print(np.max(roc_aucs))
xgbest = best_xgbs[np.argmax(roc_aucs)]
best_splits = splits[np.argmax(roc_aucs)]
# xgbest = best_xgbs[2]
# best_splits = splits[2]
```

```{python}
np.concatenate(predicted_probas_s,axis=0)
```

## Evaluation

```{python}
if type(X) == np.ndarray:
    X_test = X[best_splits[1]]
    y_test = y[best_splits[1]]
else:
    X_test = X.iloc[best_splits[1]]
    y_test = y.iloc[best_splits[1]]

predicted_proba = xgbest.predict_proba(X_test)[:, 1]

roc_auc = roc_auc_score(y_test, predicted_proba)
pr_auc = average_precision_score(y_test, predicted_proba)

print("ROC AUC: {} \n".format(roc_auc))
print("PR AUC: {} \n".format(pr_auc))
```

```{python}
prob_pos = np.concatenate(predicted_probas_s,axis=0)[0:2479]
y_true = data.got_go

brier_score_loss(y_true, prob_pos)
```

```{python}
def plot_calibration_curve(name, fig_index):
    """Plot calibration curve for est w/o and with calibration. """
    
    fig = plt.figure(fig_index, figsize=(10, 10))
    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
#     ax2 = plt.subplot2grid((3, 1), (2, 0))
    
    ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")

#     prob_pos = xgbest.predict_proba(X_test)[:, 1]
#     y_pred = xgbest.predict(X_test)
#     print("%s:" % name)
#     print("\tPrecision: %1.3f" % precision_score(y_test, y_pred))
#     print("\tRecall: %1.3f" % recall_score(y_test, y_pred))
    prob_pos = np.concatenate(predicted_probas_s,axis=0)[0:2479]
    y_test = data.got_go
    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)

    ax1.plot(mean_predicted_value, fraction_of_positives, "s-")
#              label="%s (%1.3f)" % (name, clf_score))

#     ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,
#              histtype="step", lw=2)

    ax1.set_ylabel("Fraction of positives")
    ax1.set_ylim([-0.05, 1.05])
    ax1.legend(loc="lower right")
    ax1.set_title('Calibration plots  (reliability curve)')

#     ax2.set_xlabel("Mean predicted value")
#     ax2.set_ylabel("Count")
#     ax2.legend(loc="upper center", ncol=2)

    plt.tight_layout()

# Plot calibration curve for Linear SVC
plot_calibration_curve("XGBest", 1)

plt.show()
```

```{python}
# prediction_positive = xgbest.predict_proba(X_test)[:, 1]
prob_pos = np.concatenate(predicted_probas_s,axis=0)
plt.hist(prob_pos, bins=10)
```

```{python}
X_test.shape
```

```{python}
data.head()
```

## Save data

```{python}
data["finished_treatment"] = all_data["finished_treatment"]
data_y = data.drop('got_go', axis=1)
```

```{python}
data_y.to_csv("/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/data/processed/classifier_y/weighted/sk_xgb_weights/data_classifier_y_weighted_sk_xgboost.csv", sep=";", index=False)
```

## Save Model

```{python}
# Save model
save_model(best_xgb, '/Users/steliosrammos/Documents/Education/Maastricht/DKE-Year3/BachelorThesis/bachelor_thesis/models/classifier_s/calibrated/classifier_s_sk_xgboost')
```

```{python}

```
